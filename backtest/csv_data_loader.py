"""
CSV Data Loader - Ø®ÙˆØ§Ù†Ù†Ø¯Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ø§Ø² ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ CSV
Ø§ÛŒÙ† Ù…Ø§Ú˜ÙˆÙ„ ÙˆØ¸ÛŒÙÙ‡ Ø®ÙˆØ§Ù†Ø¯Ù†ØŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ùˆ Ù…Ø¯ÛŒØ±ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ù†Ø¯Ù„ Ø§Ø² CSV Ø±Ø§ Ø¯Ø§Ø±Ø¯

ğŸ”¥ Ù†Ø³Ø®Ù‡ Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡ - Ø±ÙØ¹ Ù…Ø´Ú©Ù„Ø§Øª:
1. Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ø¨Ø§ ØªÙ…Ø§Ù… Ù†Ø³Ø®Ù‡â€ŒÙ‡Ø§ÛŒ pandas (ffill/bfill)
2. Ù…Ø¯ÛŒØ±ÛŒØª Ø¨Ù‡ØªØ± Ø®Ø·Ø§Ù‡Ø§ Ø¯Ø± _fill_missing_candles
3. Ø¨Ù‡Ø¨ÙˆØ¯ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ùˆ error handling
4. Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù† ØªØ§Ø¨Ø¹ _safe_forward_fill
"""

import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import logging
from collections import defaultdict

logger = logging.getLogger(__name__)


class CSVDataLoader:
    """
    Ú©Ù„Ø§Ø³ Ù…Ø³Ø¦ÙˆÙ„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ OHLCV Ø§Ø² ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ CSV
    """

    def __init__(self, config: Dict):
        """
        Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ CSVDataLoader

        Args:
            config: Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§Ø² Ø¨Ø®Ø´ backtest.csv_format
        """
        self.config = config
        self.backtest_config = config.get('backtest', {})
        self.csv_config = self.backtest_config.get('csv_format', {})

        # Ù…Ø³ÛŒØ± Ù¾Ø§ÛŒÙ‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        self.data_path = Path(self.backtest_config.get('data_path', './historical_data/'))

        # ğŸ”§ Ø§Ú¯Ø± Ù…Ø³ÛŒØ± Ù†Ø³Ø¨ÛŒ Ø§Ø³ØªØŒ Ø¢Ù† Ø±Ø§ Ù†Ø³Ø¨Øª Ø¨Ù‡ root Ù¾Ø±ÙˆÚ˜Ù‡ resolve Ú©Ù†
        # (root Ù¾Ø±ÙˆÚ˜Ù‡ = parent directory Ø§Ø² backtest/)
        if not self.data_path.is_absolute():
            project_root = Path(__file__).parent.parent  # Ø§Ø² csv_data_loader.py -> backtest/ -> New/
            self.data_path = (project_root / self.data_path).resolve()
            logger.info(f"Resolved relative data_path to: {self.data_path}")

        # Ù†Ú¯Ø§Ø´Øª ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…â€ŒÙ‡Ø§ Ø¨Ù‡ Ù†Ø§Ù… ÙØ§ÛŒÙ„
        self.timeframe_files = self.csv_config.get('timeframe_files', {
            '5m': '5min.csv',
            '15m': '15min.csv',
            '1h': '1hour.csv',
            '4h': '4hour.csv'
        })

        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª CSV
        self.separator = self.csv_config.get('separator', ',')
        self.encoding = self.csv_config.get('encoding', 'utf-8')
        self.date_format = self.csv_config.get('date_format', '%Y-%m-%d %H:%M:%S')

        # Ù†Ø§Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§
        self.columns = self.csv_config.get('columns', {
            'timestamp': 'timestamp',
            'open': 'open',
            'high': 'high',
            'low': 'low',
            'close': 'close',
            'volume': 'volume'
        })

        # Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯Ù‡
        self.ignore_columns = self.csv_config.get('ignore_columns', ['timestamp_unix'])

        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ
        self.validate_data = self.csv_config.get('validate_data', True)
        self.fill_missing_data = self.csv_config.get('fill_missing_data', True)
        self.remove_duplicates = self.csv_config.get('remove_duplicates', True)
        self.check_chronological = self.csv_config.get('check_chronological_order', True)

        # Ú©Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø± Ø­Ø§ÙØ¸Ù‡
        self.data_cache: Dict[str, Dict[str, pd.DataFrame]] = defaultdict(dict)

        # Ø¢Ù…Ø§Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ
        self.stats = {
            'files_loaded': 0,
            'total_rows': 0,
            'invalid_rows_removed': 0,
            'duplicates_removed': 0,
            'missing_data_filled': 0
        }

        logger.info(f"CSVDataLoader initialized with data_path: {self.data_path}")

    def load_symbol_data(self, symbol: str, timeframe: str,
                        start_date: Optional[datetime] = None,
                        end_date: Optional[datetime] = None) -> Optional[pd.DataFrame]:
        """
        Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÛŒÚ© Ù†Ù…Ø§Ø¯ Ùˆ ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ… Ø®Ø§Øµ

        Args:
            symbol: Ù†Ø§Ù… Ù†Ù…Ø§Ø¯ (Ù…Ø«Ù„Ø§Ù‹ 'BTC-USDT')
            timeframe: ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ… (Ù…Ø«Ù„Ø§Ù‹ '5m', '1h', '4h')
            start_date: ØªØ§Ø±ÛŒØ® Ø´Ø±ÙˆØ¹ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
            end_date: ØªØ§Ø±ÛŒØ® Ù¾Ø§ÛŒØ§Ù† (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)

        Returns:
            DataFrame Ø­Ø§ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ OHLCV ÛŒØ§ None Ø¯Ø± ØµÙˆØ±Øª Ø®Ø·Ø§
        """
        # Ú†Ú© Ú©Ø´
        cache_key = f"{symbol}_{timeframe}"
        if cache_key in self.data_cache[symbol]:
            df = self.data_cache[symbol][cache_key].copy()
            logger.debug(f"Loaded {symbol} {timeframe} from cache")
        else:
            # Ø³Ø§Ø®Øª Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„
            file_path = self._get_file_path(symbol, timeframe)

            if not file_path.exists():
                logger.error(f"CSV file not found: {file_path}")
                return None

            try:
                # Ø®ÙˆØ§Ù†Ø¯Ù† CSV
                df = pd.read_csv(
                    file_path,
                    sep=self.separator,
                    encoding=self.encoding
                )

                self.stats['files_loaded'] += 1
                initial_rows = len(df)
                self.stats['total_rows'] += initial_rows

                # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
                df = self._process_dataframe(df, symbol, timeframe)

                if df is None or df.empty:
                    logger.error(f"Failed to process data for {symbol} {timeframe}")
                    return None

                # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ù…Ø§Ø±
                rows_removed = initial_rows - len(df)
                if rows_removed > 0:
                    self.stats['invalid_rows_removed'] += rows_removed

                # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ú©Ø´
                self.data_cache[symbol][cache_key] = df.copy()

                logger.info(
                    f"Loaded {len(df)} candles for {symbol} {timeframe} "
                    f"from {df[self.columns['timestamp']].iloc[0]} to {df[self.columns['timestamp']].iloc[-1]}"
                )

            except Exception as e:
                logger.error(f"Error loading CSV {file_path}: {e}", exc_info=True)
                return None

        # ÙÛŒÙ„ØªØ± Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ
        if start_date or end_date:
            df = self._filter_by_date_range(df, start_date, end_date)

        return df

    def _get_file_path(self, symbol: str, timeframe: str) -> Path:
        """
        Ø³Ø§Ø®Øª Ù…Ø³ÛŒØ± Ú©Ø§Ù…Ù„ ÙØ§ÛŒÙ„ CSV

        Args:
            symbol: Ù†Ø§Ù… Ù†Ù…Ø§Ø¯
            timeframe: ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…

        Returns:
            Path object Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„
        """
        # Ø¯Ø±ÛŒØ§ÙØª Ù†Ø§Ù… ÙØ§ÛŒÙ„ Ø§Ø² Ù†Ú¯Ø§Ø´Øª
        filename = self.timeframe_files.get(timeframe)
        if not filename:
            raise ValueError(f"Unknown timeframe: {timeframe}")

        # Ø³Ø§Ø®Øª Ù…Ø³ÛŒØ±: data_path / symbol / filename
        file_path = self.data_path / symbol / filename

        return file_path

    def _process_dataframe(self, df: pd.DataFrame, symbol: str, timeframe: str) -> Optional[pd.DataFrame]:
        """
        Ù¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ DataFrame

        Args:
            df: DataFrame Ø®Ø§Ù…
            symbol: Ù†Ø§Ù… Ù†Ù…Ø§Ø¯
            timeframe: ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…

        Returns:
            DataFrame Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡ ÛŒØ§ None Ø¯Ø± ØµÙˆØ±Øª Ø®Ø·Ø§
        """
        try:
            # Ø­Ø°Ù Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯Ù‡
            for col in self.ignore_columns:
                if col in df.columns:
                    df = df.drop(columns=[col])

            # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¶Ø±ÙˆØ±ÛŒ
            required_cols = list(self.columns.values())
            missing_cols = [col for col in required_cols if col not in df.columns]
            if missing_cols:
                logger.error(f"Missing required columns for {symbol} {timeframe}: {missing_cols}")
                return None

            # ØªØ¨Ø¯ÛŒÙ„ Ø³ØªÙˆÙ† timestamp Ø¨Ù‡ datetime
            timestamp_col = self.columns['timestamp']

            try:
                df[timestamp_col] = pd.to_datetime(
                    df[timestamp_col],
                    format=self.date_format,
                    errors='coerce'
                )
            except Exception as e:
                logger.error(f"Error parsing timestamps for {symbol} {timeframe}: {e}")
                return None

            # Ø­Ø°Ù Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ø¨Ø§ timestamp Ù†Ø§Ù…Ø¹ØªØ¨Ø±
            invalid_timestamps = df[timestamp_col].isna().sum()
            if invalid_timestamps > 0:
                logger.warning(f"Removing {invalid_timestamps} rows with invalid timestamps for {symbol} {timeframe}")
                df = df.dropna(subset=[timestamp_col])

            if df.empty:
                logger.error(f"No valid data after timestamp parsing for {symbol} {timeframe}")
                return None

            # ØªØ¨Ø¯ÛŒÙ„ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
            numeric_cols = ['open', 'high', 'low', 'close', 'volume']
            for col_key in numeric_cols:
                col_name = self.columns[col_key]
                try:
                    df[col_name] = pd.to_numeric(df[col_name], errors='coerce')
                except Exception as e:
                    logger.error(f"Error converting {col_name} to numeric for {symbol} {timeframe}: {e}")
                    return None

            # Ø­Ø°Ù Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ø¨Ø§ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù†Ø§Ù…Ø¹ØªØ¨Ø±
            if self.validate_data:
                df = self._validate_ohlcv_data(df, symbol, timeframe)
                if df is None or df.empty:
                    logger.error(f"No valid data after validation for {symbol} {timeframe}")
                    return None

            # Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±ÛŒâ€ŒÙ‡Ø§
            if self.remove_duplicates:
                duplicates = df.duplicated(subset=[timestamp_col]).sum()
                if duplicates > 0:
                    logger.warning(f"Removing {duplicates} duplicate rows for {symbol} {timeframe}")
                    self.stats['duplicates_removed'] += duplicates
                    df = df.drop_duplicates(subset=[timestamp_col], keep='first')

            # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø²Ù…Ø§Ù†
            if self.check_chronological:
                df = df.sort_values(by=timestamp_col)
                df = df.reset_index(drop=True)

            # Ù¾Ø± Ú©Ø±Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú¯Ù…Ø´Ø¯Ù‡
            if self.fill_missing_data:
                df = self._fill_missing_candles(df, timeframe, symbol)

            return df

        except Exception as e:
            logger.error(f"Error processing dataframe for {symbol} {timeframe}: {e}", exc_info=True)
            return None

    def _validate_ohlcv_data(self, df: pd.DataFrame, symbol: str, timeframe: str) -> pd.DataFrame:
        """
        Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ OHLCV

        Args:
            df: DataFrame Ø¨Ø±Ø§ÛŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ
            symbol: Ù†Ø§Ù… Ù†Ù…Ø§Ø¯
            timeframe: ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…

        Returns:
            DataFrame Ù…Ø¹ØªØ¨Ø±
        """
        initial_len = len(df)

        # Ø­Ø°Ù Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ù…Ù‚Ø§Ø¯ÛŒØ± NaN Ø¯Ø§Ø±Ù†Ø¯
        df = df.dropna(subset=[
            self.columns['open'],
            self.columns['high'],
            self.columns['low'],
            self.columns['close'],
            self.columns['volume']
        ])

        # Ø¨Ø±Ø±Ø³ÛŒ Ø´Ø±Ø·: high >= low
        invalid_hl = df[self.columns['high']] < df[self.columns['low']]
        if invalid_hl.any():
            logger.warning(f"Removing {invalid_hl.sum()} rows where high < low for {symbol} {timeframe}")
            df = df[~invalid_hl]

        # Ø¨Ø±Ø±Ø³ÛŒ Ø´Ø±Ø·: high >= open, close
        invalid_high = (
            (df[self.columns['high']] < df[self.columns['open']]) |
            (df[self.columns['high']] < df[self.columns['close']])
        )
        if invalid_high.any():
            logger.warning(f"Removing {invalid_high.sum()} rows where high < open/close for {symbol} {timeframe}")
            df = df[~invalid_high]

        # Ø¨Ø±Ø±Ø³ÛŒ Ø´Ø±Ø·: low <= open, close
        invalid_low = (
            (df[self.columns['low']] > df[self.columns['open']]) |
            (df[self.columns['low']] > df[self.columns['close']])
        )
        if invalid_low.any():
            logger.warning(f"Removing {invalid_low.sum()} rows where low > open/close for {symbol} {timeframe}")
            df = df[~invalid_low]

        # Ø¨Ø±Ø±Ø³ÛŒ Ø­Ø¬Ù… Ù…Ù†ÙÛŒ
        invalid_volume = df[self.columns['volume']] < 0
        if invalid_volume.any():
            logger.warning(f"Removing {invalid_volume.sum()} rows with negative volume for {symbol} {timeframe}")
            df = df[~invalid_volume]

        # Ø¨Ø±Ø±Ø³ÛŒ Ù‚ÛŒÙ…Øªâ€ŒÙ‡Ø§ÛŒ ØµÙØ± ÛŒØ§ Ù…Ù†ÙÛŒ
        price_cols = ['open', 'high', 'low', 'close']
        for col_key in price_cols:
            col_name = self.columns[col_key]
            invalid_price = df[col_name] <= 0
            if invalid_price.any():
                logger.warning(f"Removing {invalid_price.sum()} rows with invalid {col_key} for {symbol} {timeframe}")
                df = df[~invalid_price]

        rows_removed = initial_len - len(df)
        if rows_removed > 0:
            logger.info(f"Validation removed {rows_removed} invalid rows for {symbol} {timeframe}")

        return df

    def _safe_forward_fill(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ğŸ”¥ ØªØ§Ø¨Ø¹ Ø§Ù…Ù† Ø¨Ø±Ø§ÛŒ forward fill Ú©Ù‡ Ø¨Ø§ ØªÙ…Ø§Ù… Ù†Ø³Ø®Ù‡â€ŒÙ‡Ø§ÛŒ pandas Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯

        Args:
            df: DataFrame Ø¨Ø§ index ØªÙ†Ø¸ÛŒÙ… Ø´Ø¯Ù‡

        Returns:
            DataFrame Ø¨Ø§ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù¾Ø± Ø´Ø¯Ù‡
        """
        try:
            # Ø±ÙˆØ´ 1: pandas 2.0+
            return df.ffill().bfill()
        except AttributeError:
            try:
                # Ø±ÙˆØ´ 2: pandas 1.x
                return df.fillna(method='ffill').fillna(method='bfill')
            except (TypeError, AttributeError):
                try:
                    # Ø±ÙˆØ´ 3: pandas Ù‚Ø¯ÛŒÙ…ÛŒâ€ŒØªØ±
                    return df.pad().bfill()
                except Exception:
                    # Ø±ÙˆØ´ 4: Ø¢Ø®Ø±ÛŒÙ† fallback - forward fill Ø¯Ø³ØªÛŒ
                    logger.warning("Using manual forward fill fallback")
                    return df.fillna(method='pad').fillna(method='backfill')

    def _fill_missing_candles(self, df: pd.DataFrame, timeframe: str, symbol: str) -> pd.DataFrame:
        """
        ğŸ”¥ Ù¾Ø± Ú©Ø±Ø¯Ù† Ú©Ù†Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ú¯Ù…Ø´Ø¯Ù‡ Ø¨Ø§ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ú©Ø§Ù…Ù„ Ø¨Ø§ pandas

        Args:
            df: DataFrame
            timeframe: ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…
            symbol: Ù†Ø§Ù… Ù†Ù…Ø§Ø¯ (Ø¨Ø±Ø§ÛŒ Ù„Ø§Ú¯)

        Returns:
            DataFrame Ø¨Ø§ Ú©Ù†Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾Ø± Ø´Ø¯Ù‡
        """
        if len(df) == 0:
            return df

        try:
            timestamp_col = self.columns['timestamp']

            # Ø§ÛŒØ¬Ø§Ø¯ Ø±Ù†Ø¬ Ú©Ø§Ù…Ù„ Ø²Ù…Ø§Ù†ÛŒ
            start_time = df[timestamp_col].iloc[0]
            end_time = df[timestamp_col].iloc[-1]

            # Ø¯Ø±ÛŒØ§ÙØª ÙØ±Ú©Ø§Ù†Ø³ pandas
            freq = self._timeframe_to_pandas_freq(timeframe)
            if freq is None:
                logger.warning(f"Cannot determine pandas frequency for timeframe: {timeframe}")
                return df

            # Ø§ÛŒØ¬Ø§Ø¯ DatetimeIndex Ú©Ø§Ù…Ù„
            try:
                full_range = pd.date_range(
                    start=start_time,
                    end=end_time,
                    freq=freq
                )
            except Exception as e:
                logger.error(f"Error creating date range for {symbol} {timeframe}: {e}")
                return df

            # ØªÙ†Ø¸ÛŒÙ… index
            df_indexed = df.set_index(timestamp_col)

            # Reindex Ø¨Ø§ Ø±Ù†Ø¬ Ú©Ø§Ù…Ù„
            df_reindexed = df_indexed.reindex(full_range)

            # Ø´Ù…Ø§Ø±Ø´ Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡
            missing_count = df_reindexed[self.columns['close']].isna().sum()

            if missing_count > 0:
                logger.info(f"Filling {missing_count} missing candles for {symbol} {timeframe}")
                self.stats['missing_data_filled'] += missing_count

                # ğŸ”¥ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªØ§Ø¨Ø¹ Ø§Ù…Ù† forward fill
                df_filled = self._safe_forward_fill(df_reindexed)
            else:
                df_filled = df_reindexed

            # Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† timestamp Ø¨Ù‡ Ø³ØªÙˆÙ†
            df_result = df_filled.reset_index()
            df_result = df_result.rename(columns={'index': timestamp_col})

            return df_result

        except Exception as e:
            logger.error(f"Error filling missing candles for {symbol} {timeframe}: {e}", exc_info=True)
            # Ø¯Ø± ØµÙˆØ±Øª Ø®Ø·Ø§ØŒ df Ø§ØµÙ„ÛŒ Ø±Ø§ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†
            return df

    def _timeframe_to_timedelta(self, timeframe: str) -> Optional[timedelta]:
        """
        ØªØ¨Ø¯ÛŒÙ„ Ø±Ø´ØªÙ‡ ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ… Ø¨Ù‡ timedelta

        Args:
            timeframe: Ø±Ø´ØªÙ‡ ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ… (Ù…Ø«Ù„Ø§Ù‹ '5m', '1h', '4h')

        Returns:
            timedelta object ÛŒØ§ None
        """
        mapping = {
            '1m': timedelta(minutes=1),
            '5m': timedelta(minutes=5),
            '15m': timedelta(minutes=15),
            '30m': timedelta(minutes=30),
            '1h': timedelta(hours=1),
            '4h': timedelta(hours=4),
            '1d': timedelta(days=1)
        }
        return mapping.get(timeframe)

    def _timeframe_to_pandas_freq(self, timeframe: str) -> Optional[str]:
        """
        ØªØ¨Ø¯ÛŒÙ„ Ø±Ø´ØªÙ‡ ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ… Ø¨Ù‡ ÙØ±Ú©Ø§Ù†Ø³ pandas

        Args:
            timeframe: Ø±Ø´ØªÙ‡ ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…

        Returns:
            Ø±Ø´ØªÙ‡ ÙØ±Ú©Ø§Ù†Ø³ pandas ÛŒØ§ None
        """
        mapping = {
            '1m': '1min',
            '5m': '5min',
            '15m': '15min',
            '30m': '30min',
            '1h': '1h',
            '4h': '4h',
            '1d': '1D'
        }
        return mapping.get(timeframe)

    def _filter_by_date_range(self, df: pd.DataFrame,
                              start_date: Optional[datetime],
                              end_date: Optional[datetime]) -> pd.DataFrame:
        """
        ÙÛŒÙ„ØªØ± DataFrame Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ

        Args:
            df: DataFrame
            start_date: ØªØ§Ø±ÛŒØ® Ø´Ø±ÙˆØ¹
            end_date: ØªØ§Ø±ÛŒØ® Ù¾Ø§ÛŒØ§Ù†

        Returns:
            DataFrame ÙÛŒÙ„ØªØ± Ø´Ø¯Ù‡
        """
        if df is None or df.empty:
            return df

        timestamp_col = self.columns['timestamp']

        if start_date:
            df = df[df[timestamp_col] >= start_date]

        if end_date:
            df = df[df[timestamp_col] <= end_date]

        return df

    def get_data_range(self, symbol: str, timeframe: str) -> Optional[Tuple[datetime, datetime]]:
        """
        Ø¯Ø±ÛŒØ§ÙØª Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯

        Args:
            symbol: Ù†Ø§Ù… Ù†Ù…Ø§Ø¯
            timeframe: ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…

        Returns:
            Tuple Ø§Ø² (start_date, end_date) ÛŒØ§ None
        """
        df = self.load_symbol_data(symbol, timeframe)
        if df is None or df.empty:
            return None

        timestamp_col = self.columns['timestamp']
        start_date = df[timestamp_col].iloc[0]
        end_date = df[timestamp_col].iloc[-1]

        return (start_date, end_date)

    def preload_all_data(self, symbols: List[str], timeframes: List[str]) -> bool:
        """
        Ù¾ÛŒØ´â€ŒØ¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªÙ…Ø§Ù… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø± Ú©Ø´

        Args:
            symbols: Ù„ÛŒØ³Øª Ù†Ù…Ø§Ø¯Ù‡Ø§
            timeframes: Ù„ÛŒØ³Øª ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…â€ŒÙ‡Ø§

        Returns:
            True Ø¯Ø± ØµÙˆØ±Øª Ù…ÙˆÙÙ‚ÛŒØª
        """
        logger.info(f"Preloading data for {len(symbols)} symbols and {len(timeframes)} timeframes")

        success = True
        for symbol in symbols:
            for timeframe in timeframes:
                df = self.load_symbol_data(symbol, timeframe)
                if df is None or df.empty:
                    logger.error(f"Failed to preload {symbol} {timeframe}")
                    success = False
                else:
                    logger.debug(f"Preloaded {symbol} {timeframe}: {len(df)} candles")

        logger.info(f"Preloading complete. Stats: {self.stats}")
        return success

    def get_stats(self) -> Dict:
        """
        Ø¯Ø±ÛŒØ§ÙØª Ø¢Ù…Ø§Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ

        Returns:
            Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø­Ø§ÙˆÛŒ Ø¢Ù…Ø§Ø±
        """
        return self.stats.copy()

    def clear_cache(self):
        """
        Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† Ú©Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        """
        self.data_cache.clear()
        logger.info("Data cache cleared")


class CSVDataLoader:
    """
    Ú©Ù„Ø§Ø³ Ù…Ø³Ø¦ÙˆÙ„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ OHLCV Ø§Ø² ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ CSV
    """

    def __init__(self, config: Dict):
        """
        Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ CSVDataLoader

        Args:
            config: Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§Ø² Ø¨Ø®Ø´ backtest.csv_format
        """
        self.config = config
        self.backtest_config = config.get('backtest', {})
        self.csv_config = self.backtest_config.get('csv_format', {})

        # Ù…Ø³ÛŒØ± Ù¾Ø§ÛŒÙ‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        self.data_path = Path(self.backtest_config.get('data_path', './historical_data/'))

        # ğŸ”§ Ø§Ú¯Ø± Ù…Ø³ÛŒØ± Ù†Ø³Ø¨ÛŒ Ø§Ø³ØªØŒ Ø¢Ù† Ø±Ø§ Ù†Ø³Ø¨Øª Ø¨Ù‡ root Ù¾Ø±ÙˆÚ˜Ù‡ resolve Ú©Ù†
        # (root Ù¾Ø±ÙˆÚ˜Ù‡ = parent directory Ø§Ø² backtest/)
        if not self.data_path.is_absolute():
            project_root = Path(__file__).parent.parent  # Ø§Ø² csv_data_loader.py -> backtest/ -> New/
            self.data_path = (project_root / self.data_path).resolve()
            logger.info(f"Resolved relative data_path to: {self.data_path}")

        # Ù†Ú¯Ø§Ø´Øª ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…â€ŒÙ‡Ø§ Ø¨Ù‡ Ù†Ø§Ù… ÙØ§ÛŒÙ„
        self.timeframe_files = self.csv_config.get('timeframe_files', {
            '5m': '5min.csv',
            '15m': '15min.csv',
            '1h': '1hour.csv',
            '4h': '4hour.csv'
        })

        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª CSV
        self.separator = self.csv_config.get('separator', ',')
        self.encoding = self.csv_config.get('encoding', 'utf-8')
        self.date_format = self.csv_config.get('date_format', '%Y-%m-%d %H:%M:%S')

        # Ù†Ø§Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§
        self.columns = self.csv_config.get('columns', {
            'timestamp': 'timestamp',
            'open': 'open',
            'high': 'high',
            'low': 'low',
            'close': 'close',
            'volume': 'volume'
        })

        # Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯Ù‡
        self.ignore_columns = self.csv_config.get('ignore_columns', ['timestamp_unix'])

        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ
        self.validate_data = self.csv_config.get('validate_data', True)
        self.fill_missing_data = self.csv_config.get('fill_missing_data', True)
        self.remove_duplicates = self.csv_config.get('remove_duplicates', True)
        self.check_chronological = self.csv_config.get('check_chronological_order', True)

        # Ú©Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø± Ø­Ø§ÙØ¸Ù‡
        self.data_cache: Dict[str, Dict[str, pd.DataFrame]] = defaultdict(dict)

        # Ø¢Ù…Ø§Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ
        self.stats = {
            'files_loaded': 0,
            'total_rows': 0,
            'invalid_rows_removed': 0,
            'duplicates_removed': 0,
            'missing_data_filled': 0
        }

        logger.info(f"CSVDataLoader initialized with data_path: {self.data_path}")

    def load_symbol_data(self, symbol: str, timeframe: str,
                        start_date: Optional[datetime] = None,
                        end_date: Optional[datetime] = None) -> Optional[pd.DataFrame]:
        """
        Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÛŒÚ© Ù†Ù…Ø§Ø¯ Ùˆ ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ… Ø®Ø§Øµ

        Args:
            symbol: Ù†Ø§Ù… Ù†Ù…Ø§Ø¯ (Ù…Ø«Ù„Ø§Ù‹ 'BTC-USDT')
            timeframe: ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ… (Ù…Ø«Ù„Ø§Ù‹ '5m', '1h', '4h')
            start_date: ØªØ§Ø±ÛŒØ® Ø´Ø±ÙˆØ¹ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
            end_date: ØªØ§Ø±ÛŒØ® Ù¾Ø§ÛŒØ§Ù† (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)

        Returns:
            DataFrame Ø­Ø§ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ OHLCV ÛŒØ§ None Ø¯Ø± ØµÙˆØ±Øª Ø®Ø·Ø§
        """
        # Ú†Ú© Ú©Ø´
        cache_key = f"{symbol}_{timeframe}"
        if cache_key in self.data_cache[symbol]:
            df = self.data_cache[symbol][cache_key].copy()
            logger.debug(f"Loaded {symbol} {timeframe} from cache")
        else:
            # Ø³Ø§Ø®Øª Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„
            file_path = self._get_file_path(symbol, timeframe)

            if not file_path.exists():
                logger.error(f"CSV file not found: {file_path}")
                return None

            try:
                # Ø®ÙˆØ§Ù†Ø¯Ù† CSV
                df = pd.read_csv(
                    file_path,
                    sep=self.separator,
                    encoding=self.encoding
                )

                self.stats['files_loaded'] += 1
                initial_rows = len(df)
                self.stats['total_rows'] += initial_rows

                # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
                df = self._process_dataframe(df, symbol, timeframe)

                if df is None or df.empty:
                    logger.error(f"Failed to process data for {symbol} {timeframe}")
                    return None

                # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ù…Ø§Ø±
                rows_removed = initial_rows - len(df)
                if rows_removed > 0:
                    self.stats['invalid_rows_removed'] += rows_removed

                # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ú©Ø´
                self.data_cache[symbol][cache_key] = df.copy()

                logger.info(
                    f"Loaded {len(df)} candles for {symbol} {timeframe} "
                    f"from {df['timestamp'].iloc[0]} to {df['timestamp'].iloc[-1]}"
                )

            except Exception as e:
                logger.error(f"Error loading CSV {file_path}: {e}")
                return None

        # ÙÛŒÙ„ØªØ± Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ
        if start_date or end_date:
            df = self._filter_by_date_range(df, start_date, end_date)

        return df

    def _get_file_path(self, symbol: str, timeframe: str) -> Path:
        """
        Ø³Ø§Ø®Øª Ù…Ø³ÛŒØ± Ú©Ø§Ù…Ù„ ÙØ§ÛŒÙ„ CSV

        Args:
            symbol: Ù†Ø§Ù… Ù†Ù…Ø§Ø¯
            timeframe: ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…

        Returns:
            Path object Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„
        """
        # Ø¯Ø±ÛŒØ§ÙØª Ù†Ø§Ù… ÙØ§ÛŒÙ„ Ø§Ø² Ù†Ú¯Ø§Ø´Øª
        filename = self.timeframe_files.get(timeframe)
        if not filename:
            raise ValueError(f"Unknown timeframe: {timeframe}")

        # Ø³Ø§Ø®Øª Ù…Ø³ÛŒØ±: data_path / symbol / filename
        file_path = self.data_path / symbol / filename

        return file_path

    def _process_dataframe(self, df: pd.DataFrame, symbol: str, timeframe: str) -> Optional[pd.DataFrame]:
        """
        Ù¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ DataFrame

        Args:
            df: DataFrame Ø®Ø§Ù…
            symbol: Ù†Ø§Ù… Ù†Ù…Ø§Ø¯
            timeframe: ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…

        Returns:
            DataFrame Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡ ÛŒØ§ None Ø¯Ø± ØµÙˆØ±Øª Ø®Ø·Ø§
        """
        try:
            # Ø­Ø°Ù Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯Ù‡
            for col in self.ignore_columns:
                if col in df.columns:
                    df = df.drop(columns=[col])

            # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¶Ø±ÙˆØ±ÛŒ
            required_cols = list(self.columns.values())
            missing_cols = [col for col in required_cols if col not in df.columns]
            if missing_cols:
                logger.error(f"Missing required columns: {missing_cols}")
                return None

            # ØªØ¨Ø¯ÛŒÙ„ Ø³ØªÙˆÙ† timestamp Ø¨Ù‡ datetime
            timestamp_col = self.columns['timestamp']
            df[timestamp_col] = pd.to_datetime(
                df[timestamp_col],
                format=self.date_format,
                errors='coerce'
            )

            # Ø­Ø°Ù Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ø¨Ø§ timestamp Ù†Ø§Ù…Ø¹ØªØ¨Ø±
            invalid_timestamps = df[timestamp_col].isna().sum()
            if invalid_timestamps > 0:
                logger.warning(f"Removing {invalid_timestamps} rows with invalid timestamps")
                df = df.dropna(subset=[timestamp_col])

            # ØªØ¨Ø¯ÛŒÙ„ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
            numeric_cols = ['open', 'high', 'low', 'close', 'volume']
            for col_key in numeric_cols:
                col_name = self.columns[col_key]
                df[col_name] = pd.to_numeric(df[col_name], errors='coerce')

            # Ø­Ø°Ù Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ø¨Ø§ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù†Ø§Ù…Ø¹ØªØ¨Ø±
            if self.validate_data:
                df = self._validate_ohlcv_data(df)

            # Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±ÛŒâ€ŒÙ‡Ø§
            if self.remove_duplicates:
                duplicates = df.duplicated(subset=[timestamp_col]).sum()
                if duplicates > 0:
                    logger.warning(f"Removing {duplicates} duplicate rows")
                    self.stats['duplicates_removed'] += duplicates
                    df = df.drop_duplicates(subset=[timestamp_col], keep='first')

            # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø²Ù…Ø§Ù†
            if self.check_chronological:
                df = df.sort_values(by=timestamp_col)
                df = df.reset_index(drop=True)

            # Ù¾Ø± Ú©Ø±Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú¯Ù…Ø´Ø¯Ù‡
            if self.fill_missing_data:
                df = self._fill_missing_candles(df, timeframe)

            return df

        except Exception as e:
            logger.error(f"Error processing dataframe: {e}")
            return None

    def _validate_ohlcv_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ OHLCV

        Args:
            df: DataFrame Ø¨Ø±Ø§ÛŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ

        Returns:
            DataFrame Ù…Ø¹ØªØ¨Ø±
        """
        initial_len = len(df)

        # Ø­Ø°Ù Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ù…Ù‚Ø§Ø¯ÛŒØ± NaN Ø¯Ø§Ø±Ù†Ø¯
        df = df.dropna(subset=[
            self.columns['open'],
            self.columns['high'],
            self.columns['low'],
            self.columns['close'],
            self.columns['volume']
        ])

        # Ø¨Ø±Ø±Ø³ÛŒ Ø´Ø±Ø·: high >= low
        invalid_hl = df[self.columns['high']] < df[self.columns['low']]
        if invalid_hl.any():
            logger.warning(f"Removing {invalid_hl.sum()} rows where high < low")
            df = df[~invalid_hl]

        # Ø¨Ø±Ø±Ø³ÛŒ Ø´Ø±Ø·: high >= open, close
        invalid_high = (
            (df[self.columns['high']] < df[self.columns['open']]) |
            (df[self.columns['high']] < df[self.columns['close']])
        )
        if invalid_high.any():
            logger.warning(f"Removing {invalid_high.sum()} rows where high < open/close")
            df = df[~invalid_high]

        # Ø¨Ø±Ø±Ø³ÛŒ Ø´Ø±Ø·: low <= open, close
        invalid_low = (
            (df[self.columns['low']] > df[self.columns['open']]) |
            (df[self.columns['low']] > df[self.columns['close']])
        )
        if invalid_low.any():
            logger.warning(f"Removing {invalid_low.sum()} rows where low > open/close")
            df = df[~invalid_low]

        # Ø¨Ø±Ø±Ø³ÛŒ Ø­Ø¬Ù… Ù…Ù†ÙÛŒ
        invalid_volume = df[self.columns['volume']] < 0
        if invalid_volume.any():
            logger.warning(f"Removing {invalid_volume.sum()} rows with negative volume")
            df = df[~invalid_volume]

        # Ø¨Ø±Ø±Ø³ÛŒ Ù‚ÛŒÙ…Øªâ€ŒÙ‡Ø§ÛŒ ØµÙØ± ÛŒØ§ Ù…Ù†ÙÛŒ
        price_cols = ['open', 'high', 'low', 'close']
        for col_key in price_cols:
            col_name = self.columns[col_key]
            invalid_price = df[col_name] <= 0
            if invalid_price.any():
                logger.warning(f"Removing {invalid_price.sum()} rows with invalid {col_key}")
                df = df[~invalid_price]

        rows_removed = initial_len - len(df)
        if rows_removed > 0:
            logger.info(f"Validation removed {rows_removed} invalid rows")

        return df

    def _fill_missing_candles(self, df: pd.DataFrame, timeframe: str) -> pd.DataFrame:
        """
        ğŸ”¥ Ù¾Ø± Ú©Ø±Ø¯Ù† Ú©Ù†Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ú¯Ù…Ø´Ø¯Ù‡ Ø¨Ø§ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ú©Ø§Ù…Ù„ Ø¨Ø§ pandas

        Args:
            df: DataFrame
            timeframe: ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…

        Returns:
            DataFrame Ø¨Ø§ Ú©Ù†Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾Ø± Ø´Ø¯Ù‡
        """
        if len(df) == 0:
            return df

        try:
            # ØªØ¨Ø¯ÛŒÙ„ ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ… Ø¨Ù‡ Ø¯Ù„ØªØ§ÛŒ Ø²Ù…Ø§Ù†ÛŒ
            timeframe_delta = self._timeframe_to_timedelta(timeframe)
            if timeframe_delta is None:
                logger.warning(f"Cannot determine timedelta for timeframe: {timeframe}")
                return df

            timestamp_col = self.columns['timestamp']

            # Ø§ÛŒØ¬Ø§Ø¯ Ø±Ù†Ø¬ Ú©Ø§Ù…Ù„ Ø²Ù…Ø§Ù†ÛŒ
            start_time = df[timestamp_col].iloc[0]
            end_time = df[timestamp_col].iloc[-1]

            # Ø§ÛŒØ¬Ø§Ø¯ DatetimeIndex Ú©Ø§Ù…Ù„
            freq = self._timeframe_to_pandas_freq(timeframe)
            if freq is None:
                logger.warning(f"Cannot determine pandas frequency for timeframe: {timeframe}")
                return df

            full_range = pd.date_range(
                start=start_time,
                end=end_time,
                freq=freq
            )

            # ØªÙ†Ø¸ÛŒÙ… index
            df = df.set_index(timestamp_col)

            # Reindex Ø¨Ø§ Ø±Ù†Ø¬ Ú©Ø§Ù…Ù„
            df = df.reindex(full_range)

            # Ø´Ù…Ø§Ø±Ø´ Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡
            missing_count = df[self.columns['close']].isna().sum()

            if missing_count > 0:
                logger.info(f"Filling {missing_count} missing candles with forward fill")
                self.stats['missing_data_filled'] += missing_count

                # ğŸ”¥ Ù¾Ø± Ú©Ø±Ø¯Ù† Ø¨Ø§ Ø±ÙˆØ´ Ø³Ø§Ø²Ú¯Ø§Ø± Ø¨Ø§ Ù‡Ù…Ù‡ Ù†Ø³Ø®Ù‡â€ŒÙ‡Ø§ÛŒ pandas
                try:
                    # Ø±ÙˆØ´ Ø¬Ø¯ÛŒØ¯ pandas 2.0+
                    df = df.ffill().bfill()
                except AttributeError:
                    # Ø±ÙˆØ´ Ù‚Ø¯ÛŒÙ…ÛŒ pandas < 2.0
                    try:
                        df = df.fillna(method='ffill').fillna(method='bfill')
                    except TypeError:
                        # Ø§Ú¯Ø± Ù‡ÛŒÚ†Ú©Ø¯Ø§Ù… Ú©Ø§Ø± Ù†Ú©Ø±Ø¯ØŒ Ø§Ø² forward_fill Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
                        df = df.pad().bfill()

            # Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† timestamp Ø¨Ù‡ Ø³ØªÙˆÙ†
            df = df.reset_index()
            df = df.rename(columns={'index': timestamp_col})

            return df

        except Exception as e:
            logger.error(f"Error filling missing candles: {e}")
            # Ø¯Ø± ØµÙˆØ±Øª Ø®Ø·Ø§ØŒ df Ø§ØµÙ„ÛŒ Ø±Ø§ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†
            return df

    def _timeframe_to_timedelta(self, timeframe: str) -> Optional[timedelta]:
        """
        ØªØ¨Ø¯ÛŒÙ„ Ø±Ø´ØªÙ‡ ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ… Ø¨Ù‡ timedelta

        Args:
            timeframe: Ø±Ø´ØªÙ‡ ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ… (Ù…Ø«Ù„Ø§Ù‹ '5m', '1h', '4h')

        Returns:
            timedelta object ÛŒØ§ None
        """
        mapping = {
            '1m': timedelta(minutes=1),
            '5m': timedelta(minutes=5),
            '15m': timedelta(minutes=15),
            '30m': timedelta(minutes=30),
            '1h': timedelta(hours=1),
            '4h': timedelta(hours=4),
            '1d': timedelta(days=1)
        }
        return mapping.get(timeframe)

    def _timeframe_to_pandas_freq(self, timeframe: str) -> Optional[str]:
        """
        ØªØ¨Ø¯ÛŒÙ„ Ø±Ø´ØªÙ‡ ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ… Ø¨Ù‡ ÙØ±Ú©Ø§Ù†Ø³ pandas

        Args:
            timeframe: Ø±Ø´ØªÙ‡ ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…

        Returns:
            Ø±Ø´ØªÙ‡ ÙØ±Ú©Ø§Ù†Ø³ pandas ÛŒØ§ None
        """
        mapping = {
            '1m': '1min',
            '5m': '5min',
            '15m': '15min',
            '30m': '30min',
            '1h': '1h',
            '4h': '4h',
            '1d': '1D'
        }
        return mapping.get(timeframe)

    def _filter_by_date_range(self, df: pd.DataFrame,
                              start_date: Optional[datetime],
                              end_date: Optional[datetime]) -> pd.DataFrame:
        """
        ÙÛŒÙ„ØªØ± DataFrame Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ

        Args:
            df: DataFrame
            start_date: ØªØ§Ø±ÛŒØ® Ø´Ø±ÙˆØ¹
            end_date: ØªØ§Ø±ÛŒØ® Ù¾Ø§ÛŒØ§Ù†

        Returns:
            DataFrame ÙÛŒÙ„ØªØ± Ø´Ø¯Ù‡
        """
        timestamp_col = self.columns['timestamp']

        if start_date:
            df = df[df[timestamp_col] >= start_date]

        if end_date:
            df = df[df[timestamp_col] <= end_date]

        return df

    def get_data_range(self, symbol: str, timeframe: str) -> Optional[Tuple[datetime, datetime]]:
        """
        Ø¯Ø±ÛŒØ§ÙØª Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯

        Args:
            symbol: Ù†Ø§Ù… Ù†Ù…Ø§Ø¯
            timeframe: ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…

        Returns:
            Tuple Ø§Ø² (start_date, end_date) ÛŒØ§ None
        """
        df = self.load_symbol_data(symbol, timeframe)
        if df is None or df.empty:
            return None

        timestamp_col = self.columns['timestamp']
        start_date = df[timestamp_col].iloc[0]
        end_date = df[timestamp_col].iloc[-1]

        return (start_date, end_date)

    def preload_all_data(self, symbols: List[str], timeframes: List[str]) -> bool:
        """
        Ù¾ÛŒØ´â€ŒØ¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªÙ…Ø§Ù… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø± Ú©Ø´

        Args:
            symbols: Ù„ÛŒØ³Øª Ù†Ù…Ø§Ø¯Ù‡Ø§
            timeframes: Ù„ÛŒØ³Øª ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…â€ŒÙ‡Ø§

        Returns:
            True Ø¯Ø± ØµÙˆØ±Øª Ù…ÙˆÙÙ‚ÛŒØª
        """
        logger.info(f"Preloading data for {len(symbols)} symbols and {len(timeframes)} timeframes")

        success = True
        for symbol in symbols:
            for timeframe in timeframes:
                df = self.load_symbol_data(symbol, timeframe)
                if df is None or df.empty:
                    logger.error(f"Failed to preload {symbol} {timeframe}")
                    success = False
                else:
                    logger.debug(f"Preloaded {symbol} {timeframe}: {len(df)} candles")

        logger.info(f"Preloading complete. Stats: {self.stats}")
        return success

    def get_stats(self) -> Dict:
        """
        Ø¯Ø±ÛŒØ§ÙØª Ø¢Ù…Ø§Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ

        Returns:
            Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø­Ø§ÙˆÛŒ Ø¢Ù…Ø§Ø±
        """
        return self.stats.copy()

    def clear_cache(self):
        """
        Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† Ú©Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        """
        self.data_cache.clear()
        logger.info("Data cache cleared")